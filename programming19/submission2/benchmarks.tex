\subsection{Benchmarks}
All tests presented in Section~\ref{sec:spec-minining} have been employed as benchmarks for performance evaluation
of our prototype on an Intel(R) Core(TM) i7-6500U CPU at 2.50GHz with a 16GB RAM, running SWI-Prolog 7.2.3 and Node.js v10.0.0 on
Linux kernel 4.14.40.
In particular, we have measured how much the performances of all implemented servers are affected by runtime monitoring;
only correct implementations have been considered, to be able to overload servers with arbitrary numbers of requests.
The benchmarks have shown that the overhead of runtime verification is mainly due to code instrumentation, rather than the particular monitored
specification (picked among those considered in Section~\ref{sec:spec-minining}), therefore all measurements have been conducted while verifying the same trace expression, namely,
the one corresponding to the specification \textbf{omitted body (204 response)}.

\begin{table}[ht]
  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Benchmark} &
    \textbf{Original} &
    \textbf{Monitored} &
    \textbf{Overhead} \\
    \hline
    \multicolumn{4}{|c|}{HTTP module, server-side}\\
    \hline

    Omitted body (204 response)&
    716 RPS &
    660 RPS &
    8 \% \\

    Omitted body (304 response)&
    714 RPS &
    659 RPS &
    8 \% \\

    Write head &
    708 RPS &
    596 RPS &
    19 \% \\

    Response end &
    768 RPS &
    717 RPS &
    7 \% \\

    \hline

    \multicolumn{4}{|c|}{HTTP module, client-side}\\
    \hline

    Content length &
    697 RPS &
    541 RPS &
    29 \% \\

    Omitted body (HEAD request)&
    706 RPS &
    552 RPS &
    28 \% \\

    Unconsumed data &
     780 RPS &
     633 RPS &
     23 \% \\

     \hline
    \multicolumn{4}{|c|}{Express, server-side}\\
    \hline
    Hello world &
    767 RPS &
    744 RPS &
    3 \% \\

    File explorer &
    563 RPS &
    444 RPS &
    27 \%\\

    \hline
  \end{tabular}
  \caption{Benchmark results.}
  \label{table}
\end{table}
\vspace{-1em}
Table~\ref{table} shows the results of our experiments.
For each benchmark we have measured the average number of served requests per second (RPS)
with a client running on the same machine a loop sending requests to the server without waiting to collect its response;
all measurements have been obtained as the average over ten different runs, each consisting
of a loop of 100K requests.

The second column (Original) displays the values concerning the original server, while the third one
(Monitored) reports the measurements for the same server under monitoring, interacting with the same client as in the measurements displayed in the ``Original'' column.
For several cases (content length, omitted body -- 204/304 response, hello world) the overhead is negligible;
for other servers directly implemented with the \lstinline{http} module (omitted body -- head request, unconsumed data and write head)
we registered a modest overhead (below 50\%)

For the Express file explorer presented in Section~\ref{sec:express}, the benchmarking client keeps sending GET requests
with randomly generated paths; in this case the overhead is more significant (77 \%), but still the overall performance
is acceptable: the server could operate at the rate of more than 350 RPS, while the system had to monitor more than 1.5 million events.
Such a result shows that the optimizations introduced in our prototype implementation allowed us
to get a 120x speed boost, and that runtime monitoring can be effectively employed for testing servers implemented with Express.
