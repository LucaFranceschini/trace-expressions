\subsection{Benchmarks}

All tests presented in Section~\ref{sec:spec-minining} have been employed as benchmarks for performance evaluation
of our prototype on an Intel(R) Core(TM) i7-6500U CPU at 2.50GHz with a 16GB RAM, running SWI-Prolog 7.2.3 and Node.js v10.0.0 on
Linux kernel 4.14.40.
In particular, we have measured how much the performances of all implemented servers are affected by runtime monitoring;
only correct implementations have been considered, to be able to overload servers with arbitrary numbers of requests.
The benchmarks have shown that the overhead of runtime verification is mainly due to code instrumentation, rather than the particular monitored
specification (picked among those considered in Section~\ref{sec:spec-minining}), therefore all measurements have been conducted while verifying the same trace expression, namely, 
the one corresponding to the specification \textbf{omitted body (204 response)}.

Table~\ref{table} shows the results of our experiments. 
\begin{table}[ht]
  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Benchmark} & 
    \textbf{Original} &
    \textbf{Monitored} &
    \textbf{Overhead} \\
    \hline
    \multicolumn{4}{|c|}{HTTP module}\\
    \hline
    Content length & 
    698 RPS &
    705 RPS &
    -1 \% \\

    Omitted body (HEAD request)&
    710 RPS &
    481 RPS &
    48 \% \\

    Omitted body (204 response)&
    721 RPS &
    671 RPS &
    7 \% \\

    Omitted body (304 response)&
    721 RPS &
    671 RPS &
    7 \% \\

    Unconsumed data &
     767 RPS &
     598 RPS &
     28 \% \\

    Write head &
    709 RPS &
    482 RPS &
    47 \% \\
    \hline

    \multicolumn{4}{|c|}{Express}\\
    \hline
    Hello world & 
    769 RPS &
    779 RPS &
    -1 \% \\

    File explorer &
    646 RPS &
    365 RPS &
    77 \%\\

    \hline
  \end{tabular}
  \caption{Benchmark results}
  \label{table}
\end{table}
For each benchmark we have measured the average number of served requests per second (RPS) 
with a client running on the same machine a loop sending requests to the server without waiting to collect its response;
all measurements have been obtained as the average over ten different runs, each consisting
of a loop of 100K requests.

The second column (Original) displays the values concerning the original server, while the third one
(Monitored) reports the measurements for the same server under monitoring, interacting with the same client as in the measurements displayed in the ``Original'' column.
For several cases (content length, omitted body -- 204/304 response, hello world) the overhead is negligible;
for other servers directly implemented with the \lstinline{http} module (omitted body -- head request, unconsumed data and write head)
we registered a modest overhead (below 50\%)

For the Express file explorer presented in Section~\ref{sec:express}, the benchamrking client keeps sending GET requests
with randomly generated paths; in this case the overhead is more significant (77 \%), but still the overall performance 
is acceptable: the server could operate at the rate of more than 350 RPS, while the system had to monitor more than 1.5 million events.
Such a result shows that the optimizations introduced in our prototype implementation allowed us
to get a 120x speed boost, and that runtime monitoring can be effectively employed for testing servers implemented with Express.
